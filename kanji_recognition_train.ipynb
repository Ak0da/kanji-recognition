{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a461a11-da97-4cf4-bf9b-83249734fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hub\n",
    "import random\n",
    "import math\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Callable\n",
    "import csv\n",
    "import copy\n",
    "import time\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116c5401-d0db-4875-beaa-f27bcd4c7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "nb_symbols = 2199\n",
    "\n",
    "from kanji_detection_model import kanji_detector\n",
    "\n",
    "    \n",
    "def getModel():\n",
    "    return kanji_detector()\n",
    "\n",
    "\n",
    "def testModel():\n",
    "    modelRunnable = getModel().to(device=device)\n",
    "    print(modelRunnable)\n",
    "    \n",
    "    summary1 = summary(\n",
    "        modelRunnable,\n",
    "        input_size=[\n",
    "            (20, 1, image_size, image_size)\n",
    "        ],\n",
    "        dtypes=[torch.double, torch.double],\n",
    "        depth=3\n",
    "    )\n",
    "    \n",
    "    print(summary1)\n",
    "    \n",
    "    del modelRunnable\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7d1999-312c-45dc-a59a-5b68ed1b4eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated : 0\n",
      "Reserved : 0\n",
      "Allocated : 0\n",
      "Reserved : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Allocated : \" + str(torch.cuda.memory_allocated()))\n",
    "print(\"Reserved : \" + str(torch.cuda.memory_reserved()))\n",
    "\n",
    "#testModel()\n",
    "#start = time.time()\n",
    "print(\"Allocated : \" + str(torch.cuda.memory_allocated()))\n",
    "print(\"Reserved : \" + str(torch.cuda.memory_reserved()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88cd556f-bad7-4022-baf5-d978b59475b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingPath = pathlib.Path().resolve() / \"Image_set\"\n",
    "\n",
    "train_names_list = []\n",
    "eval_names_list = []\n",
    "length_train = 0\n",
    "length_eval = 0\n",
    "\n",
    "g_dictNames2Labels = {}\n",
    "\n",
    "with open('loader_data_train.csv', 'r', encoding='UTF8') as file_train:\n",
    "    reader = csv.reader(file_train, delimiter=',')\n",
    "    next(reader)\n",
    "    length_train = sum(1 for line in file_train)\n",
    "    \n",
    "with open('loader_data_eval.csv', 'r', encoding='UTF8') as file_eval:\n",
    "    reader = csv.reader(file_eval, delimiter=',')\n",
    "    next(reader)\n",
    "    length_eval = sum(1 for line in file_eval)\n",
    "\n",
    "with open('loader_data_train.csv', 'r', encoding='UTF8') as file_train_2:\n",
    "    reader2 = csv.reader(file_train_2, delimiter=',')\n",
    "    next(reader2)\n",
    "    train_filenames_list = [\"\"]*length_train\n",
    "    for row in reader2:\n",
    "        name = row[0]\n",
    "        label = int(row[1])\n",
    "        g_dictNames2Labels[name]=label\n",
    "        train_names_list.append(name)\n",
    "    \n",
    "with open('loader_data_eval.csv', 'r', encoding='UTF8') as file_eval_2:\n",
    "    reader2 = csv.reader(file_eval_2, delimiter=',')\n",
    "    next(reader2)\n",
    "    eval_filenames_list = [\"\"]*length_eval\n",
    "    for row in reader2:\n",
    "        name = row[0]\n",
    "        label = int(row[1])\n",
    "        g_dictNames2Labels[name]=label\n",
    "        eval_names_list.append(name)\n",
    "\n",
    "random.shuffle(train_names_list)\n",
    "random.shuffle(eval_filenames_list)\n",
    "\n",
    "#picturesNames = [f for f in listdir(trainingPath) if isfile(join(trainingPath, f))]\n",
    "#g_dictNames = {name:{'name':name , 'number':int(name.split('_')[0]) , 'symbol':name.split('_')[1]} for name in picturesNames}\n",
    "#shufflePicturesNames = picturesNames.copy()\n",
    "#random.shuffle(shufflePicturesNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcee1a34-6067-4de2-af73-6a210aaa8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KanjiImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file, delimiter=',', header=0)\n",
    "        #print(self.img_labels)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        #image = read_image(img_path)\n",
    "        image = Image.open(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "training_data = KanjiImageDataset(\"loader_data_train.csv\", \".\\Training_set\", img_transform, int)\n",
    "evaluation_data = KanjiImageDataset(\"loader_data_eval.csv\", \".\\Training_set\", img_transform, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "386967c9-a5f4-4b75-931e-9e645db03ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectorIndexTrain = [0]\n",
    "selectorIndexEval = [0]\n",
    "def selectBatches(batch_size: int, isTraining: bool) -> list:\n",
    "    global selectorIndexTrain\n",
    "    global selectorIndexEval\n",
    "    global train_name_list\n",
    "    global eval_name_list\n",
    "    \n",
    "    true_list_len = len(train_names_list) if isTraining else len(eval_names_list)\n",
    "    names_list = train_names_list if isTraining else eval_names_list\n",
    "    selector_index = selectorIndexTrain if isTraining else selectorIndexEval\n",
    "    \n",
    "    labels = []\n",
    "    names = []\n",
    "    while len(labels) < batch_size:\n",
    "        list_len = true_list_len - selector_index[0]\n",
    "        n_to_find = batch_size - len(labels)\n",
    "        start = selector_index[0]\n",
    "        \n",
    "        if list_len >= n_to_find:\n",
    "            end = selector_index[0]+n_to_find\n",
    "            labels.extend([g_dictNames2Labels[name] for name in names_list[start:end]])\n",
    "            names.extend(names_list[start:end])\n",
    "            selector_index[0] += n_to_find\n",
    "        else:\n",
    "            labels.extend([g_dictNames2Labels[name] for name in names_list[start:]])\n",
    "            names.extend(names_list[start:])\n",
    "            random.shuffle(names_list)\n",
    "            selector_index[0] = 0\n",
    "    \n",
    "    #print(str(batch[0]) +\" \"+ str(batch[99]))\n",
    "    #print(batch_size)\n",
    "    #print(len(batch))\n",
    "    \n",
    "    return torch.LongTensor(labels), names\n",
    "\n",
    "def getAnswerIndices(batchList) -> torch.FloatTensor:\n",
    "    \n",
    "    correctAnswer = torch.zeros((len(batchList),nb_symbols)).float()\n",
    "    correctAnswerIndices = torch.zeros(len(batchList)).long()\n",
    "    \n",
    "    for i in range(len(batchList)):\n",
    "        #indexCorrect = batchList[i][1]['number']-1\n",
    "        indexCorrect = batchList[i]['number']-1\n",
    "        correctAnswer[i][indexCorrect] = 1\n",
    "        correctAnswerIndices[i] = indexCorrect\n",
    "    \n",
    "    \n",
    "    return correctAnswer, correctAnswerIndices\n",
    "\n",
    "\n",
    "#def countCorrect(answer: torch.FloatTensor, correctAnswer: torch.FloatTensor):\n",
    "def countCorrect(answer: torch.FloatTensor, correctAnswerIndices: torch.FloatTensor):\n",
    "    \n",
    "    _,indicesAnswer = torch.max(answer, dim=1)\n",
    "    \n",
    "    #print(indicesAnswer)\n",
    "    #print(indicesCorrect)\n",
    "    numCorrect = (indicesAnswer == correctAnswerIndices).long().sum()\n",
    "    \n",
    "    return numCorrect.item()\n",
    "\n",
    "#def countTop5Correct(answer: torch.FloatTensor, correctAnswer: torch.FloatTensor):\n",
    "def countTop5Correct(answer: torch.FloatTensor, correctAnswerIndices: torch.FloatTensor):\n",
    "    _,indicesAnswer = answer.topk(k=5, dim=1)\n",
    "    \n",
    "    #print(indicesAnswer.shape)\n",
    "    #print(correctAnswerIndices.shape)\n",
    "    numCorrect = (indicesAnswer == correctAnswerIndices.unsqueeze(-1)).long().sum()\n",
    "    \n",
    "    return numCorrect.item()\n",
    "\n",
    "def getPictures(filenames: list) -> torch.FloatTensor:\n",
    "    \n",
    "    images = torch.zeros((len(filenames), 1, 40, 40)).to(device=device)\n",
    "    labels = []\n",
    "    \n",
    "    to_tensor = transforms.ToTensor()\n",
    "    \n",
    "    convert_tensor = transforms.Compose([\n",
    "        #transforms.Grayscale(),\n",
    "        transforms.Resize(image_size)\n",
    "        #transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    i=0\n",
    "    for filename in filenames:\n",
    "        img = Image.open(trainingPath / filename)\n",
    "        #images[i,:,:,:] = convert_tensor(img)\n",
    "        images[i,:,:,:] = to_tensor(img).to(device=device)\n",
    "        i+=1\n",
    "    \n",
    "    images = convert_tensor(images)\n",
    "    return images\n",
    "\n",
    "timer1 = 0\n",
    "timer2 = 0\n",
    "timer3 = 0\n",
    "def getTensors(filenames: list) -> torch.FloatTensor:\n",
    "    global timer1\n",
    "    global timer2\n",
    "    global timer3\n",
    "    \n",
    "    start = time.time() #debug\n",
    "    images = torch.zeros((len(filenames), 1, 64, 64))#.to(device=device)\n",
    "    end = time.time() #debug\n",
    "    timer1 += end-start #debug\n",
    "    labels = []\n",
    "    \n",
    "    i=0\n",
    "    for filename in filenames:\n",
    "        \n",
    "        start = time.time() #debug\n",
    "        img = torch.load(trainingPath / filename)\n",
    "        end = time.time() #debug\n",
    "        timer2 += end-start #debug\n",
    "        \n",
    "        start = time.time() #debug\n",
    "        images[i,:,:,:] = img#.to(device=device)\n",
    "        end = time.time() #debug\n",
    "        timer3 += end-start #debug\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return (images.float()/255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b5314d7b-a246-43c3-9bd4-b84dbec96dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, n_epoch, batch_size, lr: Callable[[int], float], imagesAreTensors=False):\n",
    "    global timer1\n",
    "    global timer2\n",
    "    global timer3\n",
    "    \n",
    "    n_batches = 100\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr(0))\n",
    "\n",
    "    sampler_train = sampler=torch.utils.data.RandomSampler(training_data, replacement=False)\n",
    "    sampler_eval = sampler=torch.utils.data.RandomSampler(training_data, replacement=False)\n",
    "    \n",
    "    #dataloader_train = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    #dataloader_eval = torch.utils.data.DataLoader(evaluation_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    dataloader_train = torch.utils.data.DataLoader(training_data, batch_size=batch_size, sampler=sampler_train, num_workers=0)\n",
    "    dataloader_eval = torch.utils.data.DataLoader(evaluation_data, batch_size=batch_size, sampler=sampler_train, num_workers=0)\n",
    "    \n",
    "    frequency_detailed_results = 5\n",
    "    \n",
    "    getPicturesF = getTensors if imagesAreTensors else getPictures\n",
    "    \n",
    "    model.train()\n",
    "    loss_f = torch.nn.CrossEntropyLoss()\n",
    "    best_percent = 0\n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        n_correct_1_t = 0\n",
    "        n_correct_5_t = 0\n",
    "        n_correct_1_e = 0\n",
    "        n_correct_5_e = 0\n",
    "        \n",
    "        n_total = n_batches*batch_size\n",
    "        t_loss = 0\n",
    "        \n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr(epoch)\n",
    "        #model.zero_grad()\n",
    "            \n",
    "        time_select = 0 #debug\n",
    "        time_pictures = 0 #debug\n",
    "        time_model = 0 #debug\n",
    "        \n",
    "        start_epoch = time.time()\n",
    "            \n",
    "        print(\"Epoch \" + str(epoch+1))\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "\n",
    "            model.zero_grad()\n",
    "            #optimizer.zero_grad() not needed ?\n",
    "            \n",
    "            start = time.time() #debug\n",
    "            labels, filenames = selectBatches(batch_size, isTraining=True)\n",
    "            #images, correct_answer_indices = next(iter(dataloader_train))\n",
    "            end = time.time() #debug\n",
    "            time_select += end-start # debug\n",
    "            \n",
    "            start = time.time() #debug\n",
    "            images = getPicturesF(filenames)\n",
    "            end = time.time() #debug\n",
    "            time_pictures += end-start # debug\n",
    "            \n",
    "            #correct_answer, correct_answer_indices = getAnswerIndices(batch)\n",
    "            \n",
    "            start = time.time() #debug\n",
    "            answer = model(images.to(device=device))\n",
    "            end = time.time() #debug\n",
    "            time_model += end-start # debug\n",
    "            \n",
    "            loss = loss_f(answer,labels.to(device=device)).cpu()\n",
    "            t_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step() #Trying at the end of the epoch ?\n",
    "            \n",
    "            if (epoch+1) % frequency_detailed_results == 0:\n",
    "                #images_eval, correct_answer_indices_eval = next(iter(dataloader_eval))\n",
    "                #answer_eval = model(images_eval.to(device=device))\n",
    "                \n",
    "                labels_eval, filenames_eval = selectBatches(batch_size, isTraining=False)\n",
    "                images_eval = getPicturesF(filenames_eval)\n",
    "                answer_eval = model(images_eval.to(device=device))\n",
    "                \n",
    "                n_correct_1_t += countCorrect(answer, labels.to(device=device))\n",
    "                n_correct_5_t += countTop5Correct(answer, labels.to(device=device))\n",
    "                n_correct_1_e += countCorrect(answer_eval, labels_eval.to(device=device))\n",
    "                n_correct_5_e += countTop5Correct(answer_eval, labels_eval.to(device=device))\n",
    "            \n",
    "            \n",
    "            #print(loss.item())\n",
    "            #print(torch.softmax(answer,dim=1))\n",
    "            #print(correct_answer)\n",
    "            #print(n_correct)\n",
    "        \n",
    "        #optimizer.step()\n",
    "        adjust = 100\n",
    "        percent_1_t = math.floor(adjust*100*n_correct_1_t/n_total)/adjust\n",
    "        percent_5_t = math.floor(adjust*100*n_correct_5_t/n_total)/adjust\n",
    "        percent_1_e = math.floor(adjust*100*n_correct_1_e/n_total)/adjust\n",
    "        percent_5_e = math.floor(adjust*100*n_correct_5_e/n_total)/adjust\n",
    "        \n",
    "        display_loss = math.floor(adjust*t_loss)/adjust\n",
    "        \n",
    "        best_percent = percent_5_e if percent_5_e > best_percent else best_percent\n",
    "        \n",
    "        end_epoch = time.time()\n",
    "        time_epoch = end_epoch-start_epoch\n",
    "        \n",
    "        print(\"Time epoch : \" + str(math.floor(time_epoch*adjust)/adjust)) #debug\n",
    "        print(\"\\tTime select : \" + str(math.floor(time_select*adjust)/adjust)) #debug\n",
    "        print(\"\\tTime pictures : \" + str(math.floor(time_pictures*adjust)/adjust)) #debug\n",
    "        print(\"\\t\\tTime pictuees 1 : \" + str(math.floor(timer1*adjust)/adjust)) #debug\n",
    "        print(\"\\t\\tTime pictures 2 : \" + str(math.floor(timer2*adjust)/adjust)) #debug\n",
    "        print(\"\\t\\tTime pictures 3 : \" + str(math.floor(timer3*adjust)/adjust)) #debug\n",
    "        print(\"\\tTime model : \" + str(math.floor(time_model*adjust)/adjust)) #debug\n",
    "        \n",
    "        \n",
    "        timer1 = 0\n",
    "        timer2 = 0\n",
    "        timer3 = 0\n",
    "        \n",
    "        print(\"\\tLoss : \" + str(display_loss))\n",
    "        \n",
    "        if (epoch+1) % frequency_detailed_results == 0:\n",
    "            print(\"\\tTop-1 training accuracy : \" + str(percent_1_t) + \"%\")\n",
    "            print(\"\\tTop-5 training accuracy : \" + str(percent_5_t) + \"%\")\n",
    "            print(\"\\tTop-1 evaluation accuracy : \" + str(percent_1_e) + \"%\")\n",
    "            print(\"\\tTop-5 evaluation accuracy : \" + str(percent_5_e) + \"%\")\n",
    "        \n",
    "        if percent_5_e > 98.0:\n",
    "            break\n",
    "        \n",
    "        print(\"\")\n",
    "        \n",
    "    return best_percent\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        m.weight.data.normal_(0, 0.02)\n",
    "        m.bias.data.normal_(0, 0.001)\n",
    "    \n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        m.weight.data.normal_(0, 0.02)\n",
    "        m.bias.data.normal_(0, 0.001)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "445cd069-0390-43dc-995d-f32de01ae500",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [100] #[25, 50, 100, 150, 200]\n",
    "learning_rates = [0.00001] #[0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "27bb27e5-97c7-4a2f-9343-4d794c6c67e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor bs in batch_sizes:\\n    for lr in learning_rates:\\n        trainModel = getModel().to(device=device)\\n        weights_init(trainModel)\\n        percent = train(trainModel, n_epochs, bs, lr)\\n        print(\"bs=\" + str(bs) + \" lr=\" + str(lr) + \" : \" + str(percent) + \"%\")\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for bs in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        trainModel = getModel().to(device=device)\n",
    "        weights_init(trainModel)\n",
    "        percent = train(trainModel, n_epochs, bs, lr)\n",
    "        print(\"bs=\" + str(bs) + \" lr=\" + str(lr) + \" : \" + str(percent) + \"%\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1e18acd8-cace-4be2-b033-a74c868122d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "\n",
      "Epoch 1\n",
      "Time epoch : 6.7\n",
      "\tTime select : 0.0\n",
      "\tTime pictures : 5.59\n",
      "\t\tTime pictuees 1 : 0.02\n",
      "\t\tTime pictures 2 : 5.33\n",
      "\t\tTime pictures 3 : 0.17\n",
      "\tTime model : 0.21\n",
      "\tLoss : 769.71\n",
      "\n",
      "Epoch 2\n",
      "Time epoch : 6.52\n",
      "\tTime select : 0.0\n",
      "\tTime pictures : 5.47\n",
      "\t\tTime pictuees 1 : 0.03\n",
      "\t\tTime pictures 2 : 5.21\n",
      "\t\tTime pictures 3 : 0.14\n",
      "\tTime model : 0.15\n",
      "\tLoss : 769.59\n",
      "\n",
      "Epoch 3\n",
      "Time epoch : 6.5\n",
      "\tTime select : 0.0\n",
      "\tTime pictures : 5.46\n",
      "\t\tTime pictuees 1 : 0.03\n",
      "\t\tTime pictures 2 : 5.19\n",
      "\t\tTime pictures 3 : 0.16\n",
      "\tTime model : 0.15\n",
      "\tLoss : 769.74\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainModel = getModel().to(device=device)\n",
    "weights_init(trainModel)\n",
    "n_epochs = 3\n",
    "\n",
    "#lr: Callable[[int], float] = lambda epoch: 0.0003\n",
    "lr: Callable[[int], float] = lambda epoch: 0.0005/(1.002**epoch)\n",
    "\n",
    "print(\"Running on \" + device + \"\\n\")\n",
    "#train(trainModel, n_epochs, batch_sizes[0], learning_rates[0])\n",
    "train(trainModel, n_epochs, 100, lr, imagesAreTensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "483c9952-180c-4095-822c-5e45f278377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModel.eval()\n",
    "\n",
    "torch.save(trainModel.cpu(),\"./Models/kanji_model_v5_top5_88_eval.pt\")\n",
    "\n",
    "torch.save(trainModel.cpu().state_dict(), \"./Models/kanji_model_v5_top5_88_eval.pth\")\n",
    "\n",
    "#temp = torch.jit.script(trainModel.cpu())\n",
    "#torch.jit.save(temp, \"./Models/kanji_model_96_1.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a662c-901a-49f3-b937-6766ed490b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
